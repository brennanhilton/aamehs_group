---
title: "heirarchical_clustering"
author: "Maya Spaur"
date: "3/21/2020"
output: html_document
---
#code to read in and merge datasets copied from school_code Rmd


```{r setup}
library(tidyverse)
library(magrittr)
library(pastecs)
library(psych)
library(readxl)
library(lubridate)
library(dendextend)
library(ggdendro)
library(factoextra)
library(reshape2)
library(pals)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

# Misbath: A note on merging
# My understanding is that the DBN associated with each school is made up of 3 elements
# - the geographical district number (2 digits)
# - the borough code (a capital letter, ie K is for Brooklyn)
# - the school/building number (3 digits)

```

### Demographics
```{r, message=FALSE}
demo <- read_csv("./school_data/demographics.csv") %>% 
  janitor::clean_names() %>% 
  filter(year == "2016-17") %>% 
  mutate(school_year = year) %>% 
  select(-year) %>%
  mutate(school_name = toupper(school_name)) %>%
  drop_na(school_name)

#Removing the % sign from "percent_" columns for future analyses 
cols = c(19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 38)
demo[,cols] %<>% lapply(function(x) parse_number(x))

#The Economic Need Index (ENI) estimates the percentage of students facing economic hardship
#I also removed the % sign from that column, but need to keep in mind that it's a percentage 

#names(demo)
#str(demo)
#view(demo)

```

### Lead data
```{r, message=FALSE}
lead <- read_csv("./school_data/2017_water_lead.csv") %>% 
  janitor::clean_names() %>% 
  mutate(number_of_elevated_samples = as.numeric(number_of_elevated_samples),
         number_of_samples_tested = as.numeric(number_of_samples_tested),
         percent_elevated = 
           round((number_of_elevated_samples / number_of_samples_tested)*100, 2),
         geographical_district = as.character(geographical_district)) %>% 
#Adding leading 0 for district numbers while always having 2 digits 
  mutate(geographical_district = 
           str_pad(geographical_district, width=2, side="left", pad="0")) %>% 
  mutate(dbn = str_c(geographical_district, building_code)) %>% 
  drop_na(elevated_result) 
  
#summary(lead)
#names(lead)
#str(lead)
#view(lead)

```


### Scores
```{r, message=FALSE}
ela_scores <- read_csv("./school_data/ela_scores.csv") %>% 
  janitor::clean_names() %>% 
  filter(year %in% c("2016", "2017")) %>% 
  arrange(dbn, year) %>% 
  rename(mean_ela_score = "mean_scale_score", number_tested_ela = "number_tested") %>% 
  select(dbn:mean_ela_score)

#names(ela_scores)
#str(ela_scores)
#view(ela_scores)

math_scores <- read_csv("./school_data/math_scores.csv") %>% 
  janitor::clean_names() %>% 
  filter(year %in% c("2016", "2017")) %>%
  arrange(dbn, year) %>%
  rename(mean_math_score = "mean_scale_score", number_tested_math = "number_tested") %>% 
  select(dbn:mean_math_score)

#names(math_scores)
#str(math_scores)
#view(math_scores)

```

#adds building codes to math, ela and demo datasets
```{r, message=FALSE}
building_codes <- read_excel("./school_data/2017_water_lead.xlsx", sheet = "Building School Key", skip = 1) %>%
  janitor::clean_names() %>%
  mutate(school_name = toupper(school_name)) 


#view(building_codes)
#view(ela_scores)

ela_math <- full_join(math_scores, ela_scores) %>% 
  mutate(mean_ela_score = as.numeric(mean_ela_score),
         mean_math_score = as.numeric(mean_math_score)) %>%
  filter(grade == "All Grades") 

#view(ela_math)

codes_scores <- left_join(ela_math, building_codes, by = "school_name") %>% 
  rename(school_name_scores = school_name)
#view(codes_ela_scores)

codes_demo <- left_join(demo, building_codes, by = "school_name")
#view(codes_demo)
```

#merging files based on building code
```{r}
codes_demo_lead <- left_join(lead, codes_demo, by = "building_code") %>% 
  rename(school_name_lead = school_name)
#Adding lead data doubles our observations - something is wrong here. we have more schools tested for lead than tested for scores. E.g. buulding X has 2 schools in it, so 2 schools are assigned the lead data from testing that building. But only 1 school in the building was tested for math and ela. We needd to remove the school that was not tester by filtering so that school_name_scores == school_name_lead - the school name from the lead dataset equals the school name from the testing dataset.

#summary(codes_demo_lead)

#in some cases one school goes across multiple buildings. Summarized lead testing for each school.
codes_demo_lead_scores <- left_join(codes_demo_lead, codes_scores, by = "building_code")%>% filter(school_name_lead == school_name_scores) %>% group_by(year, school_name_scores) %>% 
  summarize(number_of_samples_tested = sum(number_of_samples_tested),
            number_of_elevated_samples = sum(number_of_elevated_samples),
            math_score = mean(mean_math_score),
            ela_score = mean(mean_ela_score)) %>% 
  mutate(percent_elevated = number_of_elevated_samples/number_of_samples_tested) %>% 
  ungroup() %>% rename(school_name=school_name_scores) 


final_data <- codes_demo_lead_scores %>% left_join(demo, by = "school_name") %>% drop_na()

#summary(final_data)

#numebrs make sense now - we have 2177 data points for test scores, and 2064 data points in our final data set. 
```

#hierarchical clustering 
https://uc-r.github.io/hc_clustering
```{r}
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
#install.packages("factoextra")
library(dendextend) # for comparing two dendrograms
```

#HC analysis: 
#Outcome: lower test scores, math_score, ela_score
#Exposure: Elevated lead concentrations in school drinking water, percent_elevated

#creating the datasets for K means, heirarchical clustering analysis

```{r}
dim(final_data)

# 1j Summary statistics on dataset

#summary(final_data)

# 1k Select the dataset

final_data_2017 =
  final_data %>%
  filter(year == "2017") %>%
  select(percent_white, percent_asian, percent_black, percent_hispanic, economic_need_index, percent_english_language_learners)

final_data_2016 =
  final_data %>%
  filter(year == "2016") %>%
  select(percent_white, percent_asian, percent_black, percent_hispanic, economic_need_index, percent_english_language_learners)

#summary(final_data_s)
#final_data_s <- as.data.frame(scale(final_data, center = TRUE))
```

#K-means
```
####*************************
#### 2: K-means Solution ####
####*************************

# 2a Set seed  
# k-means places the initial centorids at random locations, 
# if we set the seed, then each of our versions of R will 
# initialize with the same sets of starting points 

set.seed(42)

# 2b K-means with 2 clusters 
# the solution of k-means can depend on the initial locations
# of the starting points 
# so we will try multiple initial locations 
# and choose the best solution 
# The best solution is defined as the solution with the least 
# total within-cluster sum of square error 
# (between the cluster mean and each member of the cluster)

Constituents = final_data_2016_ela
km.2 <- kmeans(Constituents,
                centers = 2,   # number of clusters 
                nstart  = 100, # number of random locations to assess
                iter.max = 20) # number of iterations

# Explore nstart and iter.max

 # what happens if we only choose 1 starting position?
 sort(kmeans(Constituents, centers = 5,nstart = 1,iter.max = 20)$centers[1:5,1])
 sort(kmeans(Constituents, centers = 5,nstart = 1,iter.max = 20)$centers[1:5,1])
 # Did we arrive at different solutions? Why? Yes
 
 sort(kmeans(Constituents, centers = 5,nstart = 1,iter.max = 1)$centers[1:5,1])
 sort(kmeans(Constituents, centers = 5,nstart = 1,iter.max = 1)$centers[1:5,1])
 
 # Let's try 20 different starting positions
 sort(kmeans(Constituents, centers = 5,nstart = 20,iter.max = 20)$centers[1:5,1])
 sort(kmeans(Constituents, centers = 5,nstart = 20,iter.max = 20)$centers[1:5,1])
 # Did we arrive at different solutions? Yes
 sort(kmeans(Constituents, centers = 5,nstart = 100,iter.max = 20)$centers[1:5,1])
 sort(kmeans(Constituents, centers = 5,nstart = 100,iter.max = 20)$centers[1:5,1])
 # Did we arrive at different solutions? 
 
# 2c Features of K-means Solution  
# the k-means solution is a list with many elements
# like a regresion model or pca solution 
ls(km.2)

# size of each cluster 
km.2$size

# cluster assignment for each observation
length(km.2$cluster)
head(km.2$cluster)

# Mean concentration of each constituent 
# within each cluster
# note that these have been scaled!
km.2$centers

# total sum of squares
km.2$totss 

# within-cluster sum of squares
# for each cluster
km.2$withinss 

# Total within-cluster sum of squares
km.2$tot.withinss 
sum(km.2$withinss)

# Between-cluster sum of squares
km.2$betweenss 


# Percentage of total variance that comes from between-cluster variance
Percent_bt_k2 <- paste(round(100*(km.2$betweenss/km.2$totss),1), "%", sep = "")
Percent_bt_k2

# 2d K-means with 6 clusters 

km.6 <- kmeans(Constituents,
                centers = 6,   # number of clusters 
                nstart  = 100, # number of random locations to assess
                iter.max = 20) 

# 2e Features of K-means solution
km.6$size
head(km.6$cluster) ## cluster assignment
km.6$centers ## center means
km.6$totss ## total sum of squares
km.6$withinss ## within cluster sum of squares by cluster
km.6$tot.withinss ## total within cluster sum of squares -- this is what we want to minimize!
km.6$betweenss ## between cluster sum of squares
Percent_bt_k6 <- paste(round(100*(km.6$betweenss/km.6$totss),1), "%", sep = "")
Percent_bt_k6

# 2f Compute variance for a range of k-means solutions

# use a loop to run k-means for 0 - 50 clusters and generate variance values

km.res <- data.frame(NumClusters = rep(NA, 30), 
                     WithinSS = rep(NA, 30),
                     TotSS = rep(NA, 30))

# warning: this takes a moment to run 
KList <- seq(1,100, by = 2)

for (k in KList) {
  km.Num <- kmeans(Constituents, k, nstart = 50, iter.max = 20)
  km.res[k, ] <- cbind(k, km.Num$tot.withinss, km.Num$totss) 
}

# Create a variable for within cluster variance/total SS
km.res$PropWithin <- 100*km.res$WithinSS/km.res$TotSS

# Look at table of variances
km.res

# Plot a subset of data
km.res  %>% 
  ggplot(aes(x = NumClusters, y = PropWithin)) + geom_line() + 
  geom_vline(xintercept = 6, color = "red", linetype = "dotted") + 
  labs(title = "", 
       y = "Proportion of Within over Total SS", 
       x = "Number of Clusters")
```


```
####*******************************
#### 4: Plot K-Means Solution  ####
####*******************************

# 4a Create dataframe of means of clusters 

# 4a.i Extract cluster means from solution

km6_centers <- as.data.frame(t(km.6$centers))

# 4a.ii Create column of species names

km6_centers$species <- row.names(km6_centers)

# 4a.iii Put data in long format

plot_means_km6 <- km6_centers %>% 
                   gather(key = "Cluster", value = "mean", -species) 

# 4a.iv Rename clusters

plot_means_km6 <- plot_means_km6 %>% 
                   mutate(Cluster = fct_recode(Cluster, 
                              "Cluster 1" = "1", 
                              "Cluster 2" = "2", 
                              "Cluster 3" = "3", 
                              "Cluster 4" = "4", 
                              "Cluster 5" = "5", 
                              "Cluster 6" = "6"))

# 4.b Plot means of each cluster
# choose color palette
Color.list <- stepped(20)

ggplot(plot_means_km6, aes(x = species, y = mean, fill = species)) +
  geom_col() +
  geom_hline(yintercept = 0, size = 0.2) +
  facet_wrap(~ Cluster) +        # creates 6 plots, for each cluster
  scale_color_manual(values = Color.list) +
  scale_fill_manual(values = Color.list) +
  theme_bw()
```


```
##*****************************************************
#### 5: Create Exposure Matrix of K-Means Clusters #####
####****************************************************

# We can combine the date variable and the cluster assignment to 
# create a new exposure matrix based on which cluster the day belongs to 
# We can then use this exposure matrix in an epidemiological analysis 
# where each cluster is treated as a categorical dummy variable. 

# 5a Extract cluster membership 

clusters.df <- as.data.frame(km.6$cluster)

colnames(clusters.df) <- "cluster"

# 5b Convert clusters to character 
# unlike in PCA, the number of the clusters do not indicate order 
# they are just unique identifiers 

clusters.df$cluster <- as.factor(clusters.df$cluster)

# 5c Add the dates from the original data 

clusters.df$Date <- df0$Date

# 5d View dataframe 

head(clusters.df)

# 5e Look for weather patterns in clusters 

# 5e.i Read in weather data 

weather <- read_csv(paste0(DataFolder, "noaa_boston_weather.csv"))

# 5e.ii Organize the date column 

weather <- weather %>% dplyr::rename(Date = DATE)
weather <- weather %>% mutate(Date = parse_date_time(Date, "ymd"))

# 5e.iii Join weather data and cluster data 

df.wea <- clusters.df %>% 
          left_join(weather, by = "Date")

# 5e.iv Compute mean weather for each cluster 

summary(df.wea)

df.wea.means <- df.wea %>% 
                group_by(cluster) %>% 
                summarize(
                meanWind   = mean(AWND), 
                meanPrecip = mean(PRCP, na.rm = TRUE), 
                meanSnow   = mean(SNOW, na.rm = TRUE), 
                meanTMin   = mean(TMIN), 
                meanTMax   = mean(TMAX))

# 5e.v  Put data in long format

df.wea.means.long <- df.wea.means %>% 
                     gather(WeatherVar, "Value", -cluster)

# 5e.vi Plot average weather of each cluster 

df.wea.means.long %>% 
  ggplot(aes( cluster, Value)) + 
  geom_point(aes(color = WeatherVar), shape = 18, size = 3) + 
  facet_wrap(~WeatherVar, scales = "free")
```

#***end of K Means, now Hierarchical clustering***


#calculating Eucliean distance

```{r 2016}
#view(final_data)

# 6a.i Calculate euclidean distance between all combination of points

df.dist_2016 <- dist(final_data_2016, method = "euclidean")
df.dist_2017 <- dist(final_data_2017, method = "euclidean")

# 6a.ii Create hierarchial cluster solution
# hclust is an agglomerative clustering algorithm
# other functions are available in R 
# the method arguement determines the function used to compute distance between clusters
# We will just use the complete method for this lab. 

hc.complete_2016 <- hclust(df.dist_2016 , method = "complete")

hc.complete_2017 <- hclust(df.dist_2017, method = "complete")

# 6b Look at dendrogram

as.dendrogram(hc.complete_2016) %>% head()

as.dendrogram(hc.complete_2017) %>% head()
```

#plotting the cluster
```{r}
# hierarchial clusters are most clearly represented as visual trees

# 7a Plotting with Base R 

# 7a.i Extract the dendrogram from the HC solution 

dendro.complete_2016 <- as.dendrogram(hc.complete_2016)

dendro.complete_2017 <- as.dendrogram(hc.complete_2017)

# 7a.ii Plot 
# height indicates how dissimilar the two clusters are 
# i.e. the clusters fused at height = 22 
# are much more dissimilar 
# than the clusters fused at height = 4

dendro.complete_2016 %>% 
  plot(main = "Complete Linkage 2016 SES", ylab = "Height", leaflab = "none")

dendro.complete_2017 %>% 
  plot(main = "Complete Linkage 2017 SES", ylab = "Height", leaflab = "none")
```

#choosing the appropriate number of clusters 

```{r}
# 8a Plot branches of different cut heights
# we can use the color_branches() function to divide our data 
# into various numbers of clusters
# the dashed line (which we manually create with abline())
# indicates where the cut is taking place

#2016 
dendro.complete_2016  %>% 
  color_branches(k = 7) %>%
  plot(main = "Complete Linkage 2016 SES", ylab = "Height", leaflab = "none") %>%
abline(h = 16, lty = 3)

dendro.complete_2016  %>% 
  color_branches(k = 20) %>%
  plot(main = "Complete Linkage 2016 SES", ylab = "Height", leaflab = "none") %>%
abline(h = 16, lty = 3)

#2017
dendro.complete_2017  %>% #selected this one 
  color_branches(k = 7) %>%
  plot(main = "Complete Linkage 2017 SES", ylab = "Height", leaflab = "none") %>%
abline(h = 16, lty = 3)

dendro.complete_2017 %>% #selected this one 
  color_branches(k = 20) %>%
  plot(main = "Complete Linkage 2017 SES", ylab = "Height", leaflab = "none") %>%
abline(h = 16, lty = 3)

```


```{r}
# 8c.i Cut the tree to have 5 clusters

hc.cluster.516 <- cutree(hc.complete_2016, 5)
# this shows us the number of members in each cluster
table(hc.cluster.516)

hc.cluster.517 <- cutree(hc.complete_2017, 5)
# this shows us the number of members in each cluster
table(hc.cluster.517)

```


```{r}
# 8c.ii Include the cluster assignments in the original scaled data

df.hc.516 <- final_data_2016 %>% 
  mutate(hc.cluster.516 = hc.cluster.516) %>%
  rename("% Asian" = "percent_asian",
         "% White" = "percent_white",
         "% Black" = "percent_black",
         "% Hispanic" = "percent_hispanic",
         "Econ Need Index" = "economic_need_index",
         "% ELL" = "percent_english_language_learners"
         )

df.hc.517 <- final_data_2017 %>%
  mutate(hc.cluster.517 = hc.cluster.517) %>%
  rename("% Asian" = "percent_asian",
         "% White" = "percent_white",
         "% Black" = "percent_black",
         "% Hispanic" = "percent_hispanic",
         "Econ Need Index" = "economic_need_index",
         "% ELL" = "percent_english_language_learners"
         )


# 8c.iv Compute mean concentration of each constituent within each cluster

df.hc.mean.516 <- df.hc.516 %>% 
  group_by(hc.cluster.516) %>% 
  summarize_all(.funs = mean)

df.hc.mean.517 <- df.hc.517 %>% 
  group_by(hc.cluster.517) %>% 
  summarize_all(.funs = mean)


# 8c.v Put data in long format

plot_means_h516 <- df.hc.mean.516 %>%
  gather(key = "group", value = "mean", -hc.cluster.516) 

plot_means_h517 <- df.hc.mean.517 %>%
  gather(key = "group", value = "mean", -hc.cluster.517) 

# 8c.vii Plot means of each cluster
# we will just plot 7 of the clusters for aesthetic reasons 

# 8c.vii.i Randomly select clusters to view

#plot_means_hc7.7c <- plot_means_h7 %>% 
  #filter(hcluster.7 %in% 1:7)

# 8c.vii.ii Plot
ggplot(plot_means_h516, aes(x = group, y = mean, fill = group)) +
  geom_col() +
  geom_hline(yintercept = 0, size = 0.2) +
  facet_wrap(~ hc.cluster.516) +        # creates plots for each cluster
  theme_bw() +  theme(axis.text = element_text(angle = 90))

ggplot(plot_means_h517, aes(x = group, y = mean, fill = group)) +
  geom_col() +
  geom_hline(yintercept = 0, size = 0.2) +
  facet_wrap(~ hc.cluster.517) +        # creates plots for each cluster
  theme_bw() +  theme(axis.text = element_text(angle = 90))
 
```

#to do: assign clusters based on math and ela scores!! 

```{r}
#create datasets
final_data_2017_score =
  final_data %>%
  filter(year == "2017") %>%
  select(ela_score, math_score)

final_data_2016_score =
  final_data %>%
  filter(year == "2016") %>%
  select(ela_score, math_score)


# 6a.i Calculate euclidean distance between all combination of points

df.dist_2016 <- dist(final_data_2016_score, method = "euclidean")

df.dist_2017 <- dist(final_data_2017_score, method = "euclidean")

# 6a.ii Create hierarchial cluster solution
# hclust is an agglomerative clustering algorithm
# other functions are available in R 
# the method arguement determines the function used to compute distance between clusters
# We will just use the complete method for this lab. 

hc.complete_2016 <- hclust(df.dist_2016 , method = "complete")
hc.complete_2017 <- hclust(df.dist_2017, method = "complete")


# 7a.i Extract the dendrogram from the HC solution 

dendro.complete_2016 <- as.dendrogram(hc.complete_2016)
dendro.complete_2017 <- as.dendrogram(hc.complete_2017)


# 7a.ii Plot 
# height indicates how dissimilar the two clusters are 
# i.e. the clusters fused at height = 22 
# are much more dissimilar 
# than the clusters fused at height = 4

dendro.complete_2016 %>% 
  plot(main = "Complete Linkage 2016 Scores", ylab = "Height", leaflab = "none")

dendro.complete_2017 %>% 
  plot(main = "Complete Linkage 2017 Scores", ylab = "Height", leaflab = "none")

# 8c.i Cut the tree to have 5 clusters

hc.cluster.216 <- cutree(hc.complete_2016, 3)
# this shows us the number of members in each cluster
table(hc.cluster.216)

hc.cluster.217 <- cutree(hc.complete_2017, 3)
# this shows us the number of members in each cluster
table(hc.cluster.217)

# 8c.ii Include the cluster assignments in the original scaled data

df.hc.216 <- final_data_2016_score %>% 
  mutate(hc.cluster.216 = hc.cluster.216) 

df.hc.217 <- final_data_2017_score %>% 
  mutate(hc.cluster.217 = hc.cluster.217) 

# 8c.iv Compute mean concentration of each constituent within each cluster

df.hc.mean.216 <- df.hc.216 %>% 
  group_by(hc.cluster.216) %>% 
  summarize_all(.funs = mean)

df.hc.mean.217 <- df.hc.217 %>% 
  group_by(hc.cluster.217) %>% 
  summarize_all(.funs = mean)

# 8c.v Put data in long format

plot_means_h216 <- df.hc.mean.216 %>%
  gather(key = "group", value = "mean", -hc.cluster.216) 

plot_means_h217 <- df.hc.mean.217 %>%
  gather(key = "group", value = "mean", -hc.cluster.217) 

# 8c.vii.ii Plot
ggplot(plot_means_h216, aes(x = group, y = mean, fill = group)) +
  geom_col() +
  geom_hline(yintercept = 0, size = 0.2) +
  facet_wrap(~ hc.cluster.216) +        # creates plots for each cluster
  theme_bw() +  theme(axis.text = element_text(angle = 90))

ggplot(plot_means_h217, aes(x = group, y = mean, fill = group)) +
  geom_col() +
  geom_hline(yintercept = 0, size = 0.2) +
  facet_wrap(~ hc.cluster.217) +        # creates plots for each cluster
  theme_bw() +  theme(axis.text = element_text(angle = 90))

#not much variation was observed in test scores but 3 clusters seems better than alternatives and is consistent across the two years
```



#creating exposure matrix of heirarchical clusters

```
###*********************************************************
#### 9: Create Exposure Matrix of Hierarchial Clusters #####
####********************************************************

# We can combine the date variable and the cluster assignment to 
# create a new exposure matrix based on which cluster the day belongs to 
# We can then use this exposure matrix in an epidemiological analysis 
# where each cluster is treated as a categorical dummy variable. 

# 9a Extract cluster membership 

hc.cluster.5.final_data_2017_ela <- as.data.frame(cutree(hc.complete_2017, 5))

colnames(hc.cluster.5.final_data_2017_ela) <- "cluster"

# 9b Convert clusters to character 
# unlike in PCA, the number of the clusters do not indicate order 
# they are just unique identifiers 
# in fact, the numbers will vary each time you run kmeans()
# We will convert the clusters to letters to make sure that 
# R will treat them as dummy variables 
# and so that anyone else who looks at our data knows that they are categories 
# if you have more that 26 clusters, you will need to write a different function 

# 9c Add the dates from the original data #this step needs work

hc.cluster.5.final_data_2017_ela$mean_ela_score <- final_data$mean_ela_score

# 9d View dataframe 

head(hc.cluster.5.final_data_2017_ela)

```


```
df.wea.means.long %>% 
  ggplot(aes( cluster, Value)) + 
  geom_point(aes(color = WeatherVar), shape = 18, size = 3) + 
  facet_wrap(~WeatherVar, scales = "free")
```

